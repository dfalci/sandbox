{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_processamento_basico.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfalci/sandbox/blob/master/nlp_processamento_basico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "y4rvV_jOdWz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Expressões regulares\n",
        "\n",
        "O pacote <i><b>\"re\"</b></i> é um dos pacotes mais utilizados na linguagem Python para tratar expressões regulares.\n",
        "\n",
        "Abaixo um quadro com as suas funções mais utilizadas:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **findall** - retorna uma lista contendo todos os matches\n",
        "* **search** - Retorna um objeto match se existe algum match em algum ponto da String\n",
        "* **split** - Retorna uma lista com segmentos da String original. O ponto que determina o inicio de um novo segmento é cada novo match.\n",
        "* **sub** - Substitui um match na string por um padrão informado\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MXUjCViCdSuB",
        "colab_type": "code",
        "outputId": "50fe3f0e-6981-4273-f9ec-ac45a2b96f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "texto = 'primeiro texto'\n",
        "padrao = \"texto|teste\"\n",
        "resp = re.findall(padrao, texto)\n",
        "print resp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['texto']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZA3xEC4v9d6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Função utilitária para demonstração da API básica</b>"
      ]
    },
    {
      "metadata": {
        "id": "pgdb7QEWeTl6",
        "colab_type": "code",
        "outputId": "5a94e4d7-24c2-4911-e5f0-009ab6e93c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "def roda(padrao, texto):\n",
        "    print 'procurando no texto : {}'.format(texto)\n",
        "    print 'Resposta do findall : {}'.format(re.findall(padrao, texto))\n",
        "    print 'Resultado do split : {}'.format(re.split(padrao, texto))\n",
        "    print 'Resultado do sub : {}'.format(re.sub(padrao, '*TROCOU*', texto))\n",
        "    m = re.search(padrao, texto)\n",
        "    if not m is None:\n",
        "        print 'Resposta do search : span - {}, string - {}, group - {}'.format(m.span(), m.string, m.group())\n",
        "    \n",
        "\n",
        "roda('est', \"este e um pequeno texto de teste\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "procurando no texto : este e um pequeno texto de teste\n",
            "Resposta do findall : ['est', 'est']\n",
            "Resultado do split : ['', 'e e um pequeno texto de t', 'e']\n",
            "Resultado do sub : *TROCOU*e e um pequeno texto de t*TROCOU*e\n",
            "Resposta do search : span - (0, 3), string - este e um pequeno texto de teste, group - est\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qqC2gFuTgu7K",
        "colab_type": "code",
        "outputId": "be29c171-de44-40f5-b310-d8e16c6d21b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "#Outro teste\n",
        "roda('[aA]qui', 'Aqui vemos outro teste')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "procurando no texto : Aqui vemos outro teste\n",
            "Resposta do findall : ['Aqui']\n",
            "Resultado do split : ['', ' vemos outro teste']\n",
            "Resultado do sub : *TROCOU* vemos outro teste\n",
            "Resposta do search : span - (0, 4), string - Aqui vemos outro teste, group - Aqui\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rz0SyIOTmrpl",
        "colab_type": "code",
        "outputId": "3aba12dc-1310-413d-f9ea-1ee5dae586e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# forma alternativa\n",
        "\n",
        "\n",
        "padrao = re.compile('[aA-zZ]+')\n",
        "print padrao.findall('Um texto com mais de 10 caracteres')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Um', 'texto', 'com', 'mais', 'de', 'caracteres']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ib0zEaeS9cJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Vamos praticar!\n",
        "\n",
        "###1 - Selecione todas as cadeias alfabéticas em um texto\n",
        "<b>Apenas um teste</b>\n",
        "###2 - Selecione todas as cadeias alfabéticas (em minúscula) que terminam com a letra \"o\"\n",
        " <b>facão, jargão, melo, golo</b>\n",
        "###3 - Selecionar todas as frases que começam com um número de até 4 dígitos e terminam com uma sequência alfabética\n",
        "<b>1- Frase de teste</b>\n",
        "###4 - Selecionar todas as cadeias alfabéticas que tenham as palavras 'gato' e 'cachorro'.\n",
        "<b>eu tenho um gato e um cachorro</b>\n",
        " \n",
        "###5 - Capture uma expressão monetária em reais.\n",
        "<b>R$38,50</b>\n",
        " \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VkPlf59tBqNP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Resposta 1 - Selecione todas as cadeias alfabéticas em um texto\n",
        "<b>Apenas um teste</b>"
      ]
    },
    {
      "metadata": {
        "id": "stQc11eZn5rb",
        "colab_type": "code",
        "outputId": "7792e53e-9341-42d4-9473-3b21e92d143a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#Exercício 1\n",
        "print re.findall('[aA-zZ]+','Apenas um teste')\n",
        "#fazendo skip\n",
        "print re.findall('[aA-zZ]+','Apenas um teste, dessa vez com pontuacao e com o algarismo 3 misturado')\n",
        "#jeito modernoso de python\n",
        "print re.findall('\\w+','Apenas um teste, dessa vez com pontuacao e com o algarismo 3 misturado')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apenas', 'um', 'teste']\n",
            "['Apenas', 'um', 'teste', 'dessa', 'vez', 'com', 'pontuacao', 'e', 'com', 'o', 'algarismo', 'misturado']\n",
            "['Apenas', 'um', 'teste', 'dessa', 'vez', 'com', 'pontuacao', 'e', 'com', 'o', 'algarismo', '3', 'misturado']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "24jhobp4BtBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Resposta 2 - Selecione a cadeia alfabética (em minúscula) que termina com a letra \"o\"\n",
        " <b>facão, jargão, melo, golo</b>"
      ]
    },
    {
      "metadata": {
        "id": "wNGIX6ktBxr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b85667dd-d775-4982-9ac5-b81f4eb1237d"
      },
      "cell_type": "code",
      "source": [
        "print re.findall(r'[a-z]+o$','eu sou o melo')\n",
        "\n",
        "print re.findall(r'[a-z]+o\\b','eu so o melo')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['melo']\n",
            "['so', 'melo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nzPVkD4cDp6A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Resposta 3 - Selecionar todas as frases que começam com um número de até 4 dígitos e terminam com uma sequência alfabética\n",
        "<b>1- Frase de teste</b>"
      ]
    },
    {
      "metadata": {
        "id": "abx5aENxD3cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "26234951-775c-40b6-fe0d-a6e034e9eb3f"
      },
      "cell_type": "code",
      "source": [
        "print re.findall(r'^[0-9]+.*[aA-zZ]+$', '1 - Frase de teste')\n",
        "print re.findall(r'^[0-9]+.*[aA-zZ]+$', '1. Frase de teste')\n",
        "print re.findall(r'^[0-9]+.*[aA-zZ]+$', '200 $ Frase de teste')\n",
        "print re.findall(r'^[0-9]+.*[aA-zZ]+$', '200 $ Frase de teste.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1 - Frase de teste']\n",
            "['1. Frase de teste']\n",
            "['200 $ Frase de teste']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MNyyoniUFFkn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Resposta 4 - Selecionar todas as cadeias alfabéticas que tenham as palavras 'gato' e 'cachorro'.\n",
        "<b>eu tenho um gato e um cachorro</b>"
      ]
    },
    {
      "metadata": {
        "id": "zgZivEP7FICr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7da96e63-075e-4770-a275-9fdf89555e08"
      },
      "cell_type": "code",
      "source": [
        "print re.findall(r'gato|cachorro', 'eu tenho um gato e um cachorro')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gato', 'cachorro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9GPI14T2FkfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Resposta 5 - Capture uma expressão monetária em reais.\n",
        "<b>R$38,50</b>"
      ]
    },
    {
      "metadata": {
        "id": "ZkrtRdVGFp8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5af6ae81-8aa1-467f-a7d6-4988b9c81aff"
      },
      "cell_type": "code",
      "source": [
        "print re.findall(r'[rR]\\$[0-9]+,[0-9]{2}', 'Eu tenho r$38,50 em dinheiro')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['r$38,50']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J-t_EC3nHkiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qZV0aeeAIgK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "efb98f08-6145-4808-e292-3084064d10cc"
      },
      "cell_type": "code",
      "source": [
        "from nltk import wordpunct_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk import tokenize    \n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "print \n",
        "texto = 'Passados alguns minutos, só restou resgatar os R$50,00'\n",
        "print wordpunct_tokenize(texto)\n",
        "print word_tokenize(texto)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "['Passados', 'alguns', 'minutos', ',', 's\\xc3\\xb3', 'restou', 'resgatar', 'os', 'R', '$', '50', ',', '00']\n",
            "['Passados', 'alguns', 'minutos', ',', 's\\xc3\\xb3', 'restou', 'resgatar', 'os', 'R', '$', '50,00']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wg2BgptJK9E4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Segmentação de sentenças:\n",
        "\n",
        "A segmentação de sentenças apresenta desafios parecidos. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BlEnYNjPJ9V1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "724795e6-4671-4420-962f-85d0f748a8ad"
      },
      "cell_type": "code",
      "source": [
        "sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "frases = sent_tokenizer.tokenize(u\"\"\"O Dr. Albert Einstein é um dos maiores personagens de nossa história. Ao longo de sua vida, publicou centenas de artigos e livros\"\"\")\n",
        "print frases\n",
        "print\n",
        "for f in frases:\n",
        "    print f"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'O Dr. Albert Einstein \\xe9 um dos maiores personagens de nossa hist\\xf3ria.', u'Ao longo de sua vida, publicou centenas de artigos e livros']\n",
            "\n",
            "O Dr. Albert Einstein é um dos maiores personagens de nossa história.\n",
            "Ao longo de sua vida, publicou centenas de artigos e livros\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zt_RKYeRMxFx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vamos praticar\n",
        "\n",
        "### Dado um texto com múltiplas sentenças, crie uma funcao que realize a segmentação de sentenças, a tokenização e calcule a quantidade de tokens únicos existentes no texto"
      ]
    },
    {
      "metadata": {
        "id": "ZNgdsmEFNRiQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8f206b49-45fc-4643-9c32-c3ae722cbabb"
      },
      "cell_type": "code",
      "source": [
        "def funcao(texto):\n",
        "    sent_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "    n_sent = 0\n",
        "    n_tokens = 0\n",
        "    dicionario = {}\n",
        "    for sent in sent_tokenizer.tokenize(texto):\n",
        "        n_sent+=1\n",
        "        for w in word_tokenize(sent):\n",
        "            w = w.lower()\n",
        "            n_tokens+=1\n",
        "            dicionario[w] = dicionario.get(w, 0)+1\n",
        "    return n_sent, n_tokens, dicionario\n",
        "\n",
        "\n",
        "n_sent, n_tokens, dict = funcao(u'Este é apenas um texto. É um texto que contém várias sentenças')\n",
        "print '{} sentencas, com {} tokens e um vocabulario de {} tokens'.format(n_sent, n_tokens, len(dict))\n",
        "print '\\nAbaixo o vocabulario:\\n'\n",
        "print dict"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 sentencas, com 13 tokens e um vocabulario de 10 tokens\n",
            "\n",
            "Abaixo o vocabulario:\n",
            "\n",
            "{u'este': 1, u'senten\\xe7as': 1, u'\\xe9': 2, u'texto': 2, u'cont\\xe9m': 1, u'.': 1, u'um': 2, u'que': 1, u'v\\xe1rias': 1, u'apenas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a8HKOIsSPw7J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Stopwords\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yOMx1A-WP2n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "929497aa-414b-4073-9070-0a83bc557c1a"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u5frPowLP79l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "10e88e81-cc9d-4b86-d759-69a5c2ab8573"
      },
      "cell_type": "code",
      "source": [
        "#quantidade de stopwords em pt na stoplist do nltk\n",
        "print len(stopwords)\n",
        "\n",
        "# as 10 primeiras stopwords nessa stoplist\n",
        "print stopwords[:10]\n",
        "\n",
        "\n",
        "# removendo stopwords de  uma frase\n",
        "print [w.lower() for w in word_tokenize(u'tudo que eu puder fazer eu farei') if w.lower() not in stopwords ]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "203\n",
            "[u'de', u'a', u'o', u'que', u'e', u'do', u'da', u'em', u'um', u'para']\n",
            "[u'tudo', u'puder', u'fazer', u'farei']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UwTq9kfmSazO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Vamos praticar\n",
        "\n",
        "###Reescreva a funcao de tokenizacao feita anteriormente de forma a remover as stopwords."
      ]
    },
    {
      "metadata": {
        "id": "QJLPM0UxQbFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "a5d9f253-144a-4f33-f274-7d2547c80aaa"
      },
      "cell_type": "code",
      "source": [
        "def funcao(texto):\n",
        "    sent_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "    n_sent = 0\n",
        "    n_tokens = 0\n",
        "    dicionario = {}\n",
        "    for sent in sent_tokenizer.tokenize(texto):\n",
        "        n_sent+=1\n",
        "        for w in word_tokenize(sent):\n",
        "            if not w in stopwords:\n",
        "                w = w.lower()\n",
        "                n_tokens+=1\n",
        "                dicionario[w] = dicionario.get(w, 0)+1\n",
        "            else:\n",
        "                print 'removendo {}'.format(w)\n",
        "    print\n",
        "    return n_sent, n_tokens, dicionario\n",
        "\n",
        "\n",
        "n_sent, n_tokens, dict = funcao(u'Este é apenas um texto. É um texto que contém várias sentenças')\n",
        "print 'Temos {} sentencas, com {} tokens e um vocabulario de {} tokens'.format(n_sent, n_tokens, len(dict))\n",
        "print '\\nAbaixo o vocabulario:\\n'\n",
        "print dict"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "removendo um\n",
            "removendo um\n",
            "removendo que\n",
            "\n",
            "Temos 2 sentencas, com 10 tokens e um vocabulario de 8 tokens\n",
            "\n",
            "Abaixo o vocabulario:\n",
            "\n",
            "{u'este': 1, u'senten\\xe7as': 1, u'\\xe9': 2, u'texto': 2, u'v\\xe1rias': 1, u'.': 1, u'cont\\xe9m': 1, u'apenas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4nKrvRgi2WkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ]
    },
    {
      "metadata": {
        "id": "ArYrBdn-V_Yr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O NLTK vem com o stemmer de orengo implementado.\n",
        "\n",
        "\n",
        "Clique [aqui](http://www.inf.ufrgs.br/~viviane/rslp/) para ver a especificação"
      ]
    },
    {
      "metadata": {
        "id": "4PZ4ZIpBWPeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e7401f67-7676-45ff-d2af-6f7951121af9"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('rslp')\n",
        "\n",
        "# removedor de sufixos da lingua portuguesa - RSLP\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "stemmer.stem('Apenas')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'apen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "metadata": {
        "id": "WodRVyQv2ns7",
        "colab_type": "code",
        "outputId": "5ce2b19d-50e1-4057-985f-088d5ad814ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "padrao = 'Assim que se recuperar da cirurgia que retirou a bolsa de colostomia, o presidente Jair Bolsonaro vai chamar as bancadas e lideres de partidos para discutir os ajustes finais da reforma da Previdencia.'\n",
        "retorno = ' '.join([stemmer.stem(item) for item in padrao.split(' ')])\n",
        "print retorno\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assim que se recuper da cirurg que retir a bols de colostomia, o presid jair bolsonar vai cham as banc e lid de part par discut os ajust final da reform da previdencia.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aIPx0CkeWl_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Alternativas"
      ]
    },
    {
      "metadata": {
        "id": "0tPKZOJU2w7k",
        "colab_type": "code",
        "outputId": "6424b726-cdae-41cc-8dee-0edfb0fd3e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "print ' '.join([porter.stem(w) for w in word_tokenize('The slow advance of aid toward impoverished Venezuela has become a proxy measure of the power struggle between its two rival presidents. At the same time, there is little doubt that the Venezuelan people are in need of help. So why is it so hard to agree on aid?')])\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the slow advanc of aid toward impoverish venezuela ha becom a proxi measur of the power struggl between it two rival presid . At the same time , there is littl doubt that the venezuelan peopl are in need of help . So whi is it so hard to agre on aid ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9K1RehGBWpMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34737898-b2a8-41e3-a9f0-3bee9320e30d"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "print(\" \".join(SnowballStemmer.languages))\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rLRWU1CMWvWB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "354be5e9-53e6-4c2f-b388-e671fd011bd1"
      },
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer('portuguese')\n",
        "padrao = 'Assim que se recuperar da cirurgia que retirou a bolsa de colostomia, o presidente Jair Bolsonaro vai chamar as bancadas e lideres de partidos para discutir os ajustes finais da reforma da Previdencia.'\n",
        "retorno = ' '.join([snowball.stem(item) for item in padrao.split(' ')])\n",
        "print retorno"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assim que se recuper da cirurg que retir a bols de colostomia, o president jair bolsonar vai cham as banc e lid de part par discut os ajust fin da reform da previdencia.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fFOa6ulPfp7p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vamos praticar\n",
        "###Introduza o stemming na função de tokenizacao"
      ]
    },
    {
      "metadata": {
        "id": "RdNYaQttXtH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lemmatizer\n",
        "\n",
        "### Não existe lematizador da lingua portuguesa no NLTK. Em inglês:\n"
      ]
    },
    {
      "metadata": {
        "id": "gskp5gyqXzdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b01d3baa-0ad6-4156-ab89-aee2c3a750ae"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemzer = WordNetLemmatizer()\n",
        "\n",
        "words = [u'loved', u'loves', u'loving']\n",
        "print [lemzer.lemmatize(w, pos='v') for w in words]\n",
        "\n",
        "print [lemzer.lemmatize(w) for w in words]\n",
        "\n",
        "print [lemzer.lemmatize(w, pos='n') for w in words]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[u'love', u'love', u'love']\n",
            "[u'loved', u'love', u'loving']\n",
            "[u'loved', u'love', u'loving']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Xr1GhxKZKkq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No <b>SpaCy</b> existe suporte para a língua Portuguesa. Trata-se de um lematizador baseado em um lookup table gigantesco! Abordaremos esta biblioteca com mais profundidade posteriormente."
      ]
    },
    {
      "metadata": {
        "id": "KNPI-8KqaevV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e425907b-6bfa-4044-b115-bed62b782803"
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python2.7/dist-packages/pt_core_news_sm -->\n",
            "    /usr/local/lib/python2.7/dist-packages/spacy/data/pt\n",
            "\n",
            "    You can now load the model via spacy.load('pt')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ju3OUBrmZQGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f64183e-532c-41ea-eef6-0a759a56b31f"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"pt\")\n",
        "\n",
        "texto = u\"Esta noite, o presidente deverá convocar a bancada\"\n",
        "\n",
        "print [token.lemma_ for token in nlp(texto)]"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'Esta', u'noite', u',', u'o', u'presidente', u'dever', u'convocar', u'o', u'bancada']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zuZW2sBOS6C0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Distância de edição"
      ]
    },
    {
      "metadata": {
        "id": "i0bS8MMS6Fmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "33d520ae-0345-4ddf-dbed-d3a5d16043cb"
      },
      "cell_type": "code",
      "source": [
        "def hamming(s1, s2):\n",
        "    if len(s1) == len(s2):\n",
        "        cont = 0\n",
        "        for i in xrange(0, len(s2)):\n",
        "            if s1[i]!=s2[i]:\n",
        "                cont +=1\n",
        "        return cont\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def lev(s1, s2):\n",
        "    if len(s1) > len(s2):\n",
        "        temp = s1\n",
        "        s1 = s2\n",
        "        s2 = temp\n",
        "    if len(s1) == 0 or len(s2) == 0:\n",
        "        return max(len(s1), len(s2))\n",
        "    custo = 0 if s1[-1] == s2[-1] else 1\n",
        "\n",
        "    primeiro = lev(s1[:-1], s2)+1\n",
        "    segundo = lev(s1, s2[:-1])+1\n",
        "    terceiro = lev(s1[:-1], s2[:-1])+custo\n",
        "    return min(primeiro, segundo, terceiro)\n",
        "\n",
        "print lev('testado', 'teste')\n",
        "print hamming('teste', 'testa')\n",
        "\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mra0GHIkfkLy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
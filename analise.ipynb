{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificação de tópicos quentes na computação através da análise em titulos de artigos.\n",
    "\n",
    "Modelo ainda preliminar. Tentar utilizar os modelos LDA e LSI de forma conjunta, produzindo tópicos híbridos. Tentar obter palavras chave do artigo... mais eficiente\n",
    "\n",
    "Primeiro fazemos os imports - nltk (porter stemmer e stopwords)) e gensim (modelagem de tópicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gensim\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, TfidfModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "\n",
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parseando a dblp\n",
    "\n",
    "As classes abaixo são responsáveis por ler a dblp, extraindo para cada um de seus artigos o nome, o titulo, o ano e os autores. O campo title armazena o array de tokens utilizados com o devido pré-processamento (lowercase, split, remoção de stopwords e finalmente stemming). Note que o tempo de parsing da dblp é pequeno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "finalizado em 22.9328019619\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "class Artigo(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.year = None\n",
    "        self.title = None\n",
    "        self.journal = None\n",
    "        self.author = []\n",
    "        self.originalTitle = None\n",
    "        \n",
    "    def add_author(self,author):\n",
    "        self.author.append(author.decode('utf8').replace('<author>', '').replace('</author>', '').strip())\n",
    "        \n",
    "    def add_year(self, year):\n",
    "        self.year = year.decode('utf8').replace('<year>', '').replace('</year>', '').strip()\n",
    "    \n",
    "    def add_title(self, title):\n",
    "        self.originalTitle = title.decode('utf8').replace('<title>', '').replace('</title>', '').strip()\n",
    "        texto = gensim.utils.simple_preprocess(self.originalTitle, deacc=True, min_len=2)\n",
    "        texto = [stemmer.stem(word) for word in texto if word not in stops]     \n",
    "        self.title = texto\n",
    "        \n",
    "    \n",
    "    def add_journal(self, journal):\n",
    "        self.journal = journal.decode('utf8').replace('<journal>', '').replace('</journal>', '').strip()\n",
    "    \n",
    "    def validate(self):\n",
    "        return not self.year is None and not self.title is None and not self.journal is None\n",
    "    \n",
    "class Artigos(object):\n",
    "    \n",
    "    def __init__(self, arr=[]):\n",
    "        self.artigos = arr\n",
    "        \n",
    "        \n",
    "    def add_artigo(self, artigo):\n",
    "        self.artigos.append(artigo)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for artigo in self.artigos:\n",
    "            yield artigo\n",
    "            \n",
    "    def get_textos(self):\n",
    "        for artigo in self.artigos:\n",
    "            yield artigo.title\n",
    "            \n",
    "    def aplicar_bigramas(self):\n",
    "        self.bigram = gensim.models.Phrases(artigos.get_textos())\n",
    "        for artigo in self.artigos:\n",
    "            artigo.title = self.bigram[artigo.title]\n",
    "    \n",
    "    def get_training_text(self):\n",
    "        arr = []\n",
    "        for texto in self.get_textos():\n",
    "            arr.append(texto)\n",
    "        return arr\n",
    "            \n",
    "    \n",
    "def extract_artigos(file=\"/Volumes/My Passport/phd/dblp.xml\"):\n",
    "    artigos = Artigos()\n",
    "    with open(file, \"r\") as ins:\n",
    "        i = 0\n",
    "        artigoAtual = None\n",
    "        for line in ins:\n",
    "            if '<article' in line:    \n",
    "                if not artigoAtual is None and artigoAtual.validate():\n",
    "                    artigos.add_artigo(artigoAtual)\n",
    "                artigoAtual = Artigo()\n",
    "                i+=1\n",
    "                if i % 1000 == 0:\n",
    "                    sys.stdout.write(\"\\r\" + str(i))\n",
    "                    sys.stdout.flush()\n",
    "                if i > 60000:\n",
    "                    break\n",
    "            else:\n",
    "                if line.startswith('<author>'):\n",
    "                    artigoAtual.add_author(line)\n",
    "                if line.startswith('<year>'):\n",
    "                    artigoAtual.add_year(line)\n",
    "                if line.startswith('<title>'):\n",
    "                    artigoAtual.add_title(line)\n",
    "                if line.startswith('<journal>'):\n",
    "                    artigoAtual.add_journal(line)\n",
    "\n",
    "    ins.close()\n",
    "    return artigos\n",
    "\n",
    "ts = time.time()\n",
    "artigos = extract_artigos()\n",
    "te = time.time()\n",
    "\n",
    "print ''\n",
    "print 'finalizado em {tempo}'.format(tempo=(te-ts))\n",
    "print len(artigos.artigos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa identificamos os brigramas existentes, efetuando o merge diretamente no campo title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artigos.aplicar_bigramas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo vemos os dez primeiros exemplos de titulos originais e o resultado final da nossa tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Integer Sorting and Simulation Amongst CRCW Models.\n",
      "[u'parallel', u'integ', u'sort', u'simul', u'amongst', u'crcw', u'model']\n",
      "\n",
      "Pattern Matching in Trees and Nets.\n",
      "[u'pattern_match', u'tree', u'net']\n",
      "\n",
      "NP-complete Problems Simplified on Tree Schemas.\n",
      "[u'np_complet', u'problem', u'simplifi', u'tree', u'schema']\n",
      "\n",
      "On the Power of Chain Rules in Context Free Grammars.\n",
      "[u'power', u'chain', u'rule', u'context_free', u'grammar']\n",
      "\n",
      "Schnelle Multiplikation von Polynomen &uuml;ber K&ouml;rpern der Charakteristik 2.\n",
      "[u'schnell', u'multiplik', u'von', u'polynomen', u'uuml', u'ber', u'ouml', u'rpern', u'der', u'charakteristik']\n",
      "\n",
      "A characterization of rational D0L power series.\n",
      "[u'ration', u'power', u'seri']\n",
      "\n",
      "The Derivation of Systolic Implementations of Programs.\n",
      "[u'deriv', u'systol', u'implement', u'program']\n",
      "\n",
      "Fifo Nets Without Order Deadlock.\n",
      "[u'fifo', u'net', u'without', u'order', u'deadlock']\n",
      "\n",
      "On the Complementation Rule for Multivalued Dependencies in Database Relations.\n",
      "[u'complement', u'rule', u'multivalu', u'depend', u'databas', u'relat']\n",
      "\n",
      "Equational weighted tree transformations.\n",
      "[u'equat', u'weight', u'tree', u'transform']\n",
      "\n",
      "Merged processes: a new condensed representation of Petri net behaviour.\n",
      "[u'merg', u'process', u'new', u'condens', u'represent', u'petri_net', u'behaviour']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "for artigo in artigos:\n",
    "    print artigo.originalTitle\n",
    "    print artigo.title\n",
    "    print \n",
    "    i+=1\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo os bigramas encontrados ao longo do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_parallel\n",
      "signal_concept\n",
      "network_variat\n",
      "touch_dynam\n",
      "contrast_polar\n",
      "accuraci_trade\n",
      "reduct_variabl\n",
      "use_zone\n",
      "ray_fluoresc\n",
      "parametr_maximum\n",
      "scheme_without\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "for item in artigos.bigram.vocab:\n",
    "    if '_' in item:\n",
    "        print item\n",
    "    i+=1\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "# ver depois\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelagem de tópicos\n",
    "\n",
    "\n",
    "Agora capturamos os titulos dos artigos em um array de treinamento da modelagem de tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'parallel', u'integ', u'sort', u'simul', u'amongst', u'crcw', u'model'], [u'pattern_match', u'tree', u'net']]\n"
     ]
    }
   ],
   "source": [
    "training_texts = artigos.get_training_text()\n",
    "print training_texts[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando os dicionarios e corpus devidamente convertido para a sua representação numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(training_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in training_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho, treinamos o modelo LDA (Latent Dirichlet Allocation). Note que para um subset com 60000 titulos de artigos, o tempo de treinamento foi de apenas 3 minutos em uma máquina comum (i7 com 8gb de ram). \n",
    "\n",
    "A quantidade de tópicos precisa ser informada de forma a maximizar a coerência do corpus. Desta forma, múltiplos experimentos devem ser executados de forma a encontrar o valor ideal. Após alguns testes preliminares, decidimos utilizar 150 tópicos.\n",
    "\n",
    "O resultado do modelo é registrado em arquivo próprio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo treinado em 209.071384192\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "ldaModel = LdaModel(corpus=corpus, id2word=dictionary, num_topics=150)\n",
    "ldaModel.save('/Volumes/My Passport/phd/ldamodel')\n",
    "te = time.time()\n",
    "print 'modelo treinado em {tempo}'.format(tempo=(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldaModel = LdaModel.load('/Volumes/My Passport/phd/ldamodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo medimos a coerencia do modelo com os dados do próprio treinamento. Em seguida, listamos as principais palavras existentes em cada tópico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34835663341270884\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(model=ldaModel, corpus=corpus, dictionary=dictionary, texts=training_texts)\n",
    "print cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(125,\n",
       "  u'0.132*\"analysi\" + 0.103*\"uncertainti\" + 0.102*\"decentr\" + 0.092*\"cloud\" + 0.058*\"driven\" + 0.048*\"execut\" + 0.046*\"critic\" + 0.039*\"record\" + 0.034*\"model\" + 0.031*\"privat\"'),\n",
       " (63,\n",
       "  u'0.108*\"threshold\" + 0.098*\"compon\" + 0.088*\"noisi\" + 0.072*\"norm\" + 0.054*\"famili\" + 0.038*\"mous\" + 0.031*\"affect\" + 0.030*\"vs\" + 0.026*\"base\" + 0.022*\"use\"'),\n",
       " (18,\n",
       "  u'0.230*\"multivari\" + 0.213*\"knowledg\" + 0.137*\"system\" + 0.032*\"model\" + 0.027*\"taxonomi\" + 0.020*\"do\" + 0.012*\"musculoskelet\" + 0.011*\"analysi\" + 0.010*\"dead_time\" + 0.009*\"data\"'),\n",
       " (32,\n",
       "  u'0.191*\"random\" + 0.170*\"fingerprint\" + 0.122*\"semant\" + 0.035*\"align\" + 0.031*\"analysi\" + 0.027*\"java\" + 0.021*\"base\" + 0.017*\"system\" + 0.016*\"minutia\" + 0.016*\"model\"'),\n",
       " (52,\n",
       "  u'0.122*\"understand\" + 0.082*\"gpu\" + 0.044*\"use\" + 0.041*\"correspond\" + 0.035*\"compress_sens\" + 0.035*\"gate\" + 0.033*\"post\" + 0.032*\"scenario\" + 0.025*\"cerebr\" + 0.024*\"asymptot_stabil\"'),\n",
       " (148,\n",
       "  u'0.104*\"extend\" + 0.054*\"hash\" + 0.040*\"petri_net\" + 0.040*\"formul\" + 0.039*\"modern\" + 0.038*\"pipelin\" + 0.029*\"analog\" + 0.026*\"agrav\" + 0.022*\"high_level\" + 0.020*\"modif\"'),\n",
       " (12,\n",
       "  u'0.194*\"constrain\" + 0.151*\"multi_agent\" + 0.098*\"system\" + 0.074*\"traffic\" + 0.060*\"optim\" + 0.059*\"alloc\" + 0.046*\"expert_system\" + 0.026*\"optimum\" + 0.023*\"circular\" + 0.022*\"bia\"'),\n",
       " (77,\n",
       "  u'0.483*\"approach\" + 0.099*\"base\" + 0.080*\"autom\" + 0.059*\"system\" + 0.042*\"rule\" + 0.029*\"use\" + 0.024*\"trajectori\" + 0.019*\"cultur\" + 0.016*\"featur_extract\" + 0.016*\"pattern_recognit\"'),\n",
       " (55,\n",
       "  u'0.143*\"cach\" + 0.087*\"reliabl\" + 0.061*\"system\" + 0.050*\"simultan\" + 0.041*\"guarante\" + 0.040*\"tune\" + 0.032*\"lyapunov\" + 0.028*\"optim\" + 0.026*\"un\" + 0.022*\"sum\"'),\n",
       " (93,\n",
       "  u'0.178*\"match\" + 0.099*\"mri\" + 0.091*\"convex\" + 0.059*\"find\" + 0.044*\"live\" + 0.034*\"paper\" + 0.023*\"paradigm\" + 0.021*\"lti_system\" + 0.021*\"four\" + 0.020*\"algorithm\"'),\n",
       " (89,\n",
       "  u'0.638*\"design\" + 0.066*\"product\" + 0.047*\"optim\" + 0.034*\"scene\" + 0.034*\"clinic\" + 0.022*\"configur\" + 0.019*\"consid\" + 0.011*\"near_optim\" + 0.009*\"fals\" + 0.005*\"embodi\"'),\n",
       " (29,\n",
       "  u'0.170*\"test\" + 0.156*\"machin\" + 0.077*\"sequenti\" + 0.074*\"dimension\" + 0.054*\"facial\" + 0.033*\"cover\" + 0.032*\"model\" + 0.024*\"behaviour\" + 0.022*\"rf\" + 0.018*\"dc\"'),\n",
       " (62,\n",
       "  u'0.498*\"network\" + 0.138*\"architectur\" + 0.124*\"power\" + 0.039*\"wireless_sensor\" + 0.035*\"sequenc\" + 0.028*\"bayesian\" + 0.020*\"optim\" + 0.016*\"cascad\" + 0.014*\"system\" + 0.006*\"checkpoint\"'),\n",
       " (44,\n",
       "  u'0.369*\"function\" + 0.065*\"spline\" + 0.050*\"use\" + 0.050*\"core\" + 0.033*\"screen\" + 0.028*\"beyond\" + 0.025*\"enforc\" + 0.016*\"reactor\" + 0.016*\"real_world\" + 0.015*\"trigger\"'),\n",
       " (6,\n",
       "  u'0.186*\"class\" + 0.133*\"environ\" + 0.087*\"correl\" + 0.068*\"sensit\" + 0.050*\"autonom\" + 0.044*\"static\" + 0.030*\"analysi\" + 0.027*\"system\" + 0.025*\"infinit\" + 0.023*\"temperatur\"'),\n",
       " (71,\n",
       "  u'0.184*\"cluster\" + 0.136*\"iter\" + 0.113*\"assist\" + 0.093*\"role\" + 0.060*\"futur\" + 0.059*\"forc\" + 0.035*\"refin\" + 0.027*\"algorithm\" + 0.026*\"data\" + 0.020*\"cellular_network\"'),\n",
       " (120,\n",
       "  u'0.123*\"compress\" + 0.069*\"associ\" + 0.043*\"attitud\" + 0.038*\"base\" + 0.038*\"coher\" + 0.038*\"express\" + 0.037*\"boolean\" + 0.031*\"toler\" + 0.028*\"system\" + 0.025*\"friction\"'),\n",
       " (13,\n",
       "  u'0.211*\"order\" + 0.075*\"electron\" + 0.070*\"mean\" + 0.054*\"achiev\" + 0.053*\"server\" + 0.044*\"empir_studi\" + 0.040*\"muscl\" + 0.029*\"emiss\" + 0.026*\"pin\" + 0.024*\"data_center\"'),\n",
       " (116,\n",
       "  u'0.159*\"model\" + 0.110*\"differenti\" + 0.083*\"state_estim\" + 0.050*\"use\" + 0.040*\"registr\" + 0.031*\"circl\" + 0.031*\"tissu\" + 0.030*\"smart_grid\" + 0.024*\"neighborhood\" + 0.024*\"skin\"'),\n",
       " (99,\n",
       "  u'0.251*\"multi\" + 0.149*\"represent\" + 0.094*\"error\" + 0.066*\"identifi\" + 0.059*\"vector\" + 0.039*\"polici\" + 0.034*\"base\" + 0.026*\"use\" + 0.024*\"decis_support\" + 0.019*\"score\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaModel.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho abaixo verifica para cada documento os tópicos aos quais ele está associado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 31, 53, 87, 119]\n",
      "[49, 131, 148]\n",
      "[31, 32, 104, 131, 134]\n",
      "[49, 62, 65, 69, 77]\n",
      "[13, 66, 115, 135, 139]\n",
      "[25, 35, 62]\n",
      "[26, 53, 54, 101]\n",
      "[4, 13, 49, 102, 110]\n",
      "[11, 77, 82, 97, 149]\n",
      "[72, 103, 108, 131]\n"
     ]
    }
   ],
   "source": [
    "outro_bow = [dictionary.doc2bow(text) for text in training_texts[0:10]]\n",
    "probGrupo = ldaModel[outro_bow]\n",
    "for prob in probGrupo:\n",
    "    grupos = []\n",
    "    for item in prob:\n",
    "        grupos.append(item[0])\n",
    "    print grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'featur', 0.2602153178916348), (u'use', 0.14999187482628407), (u'base', 0.07220998516021344), (u'tree', 0.0656962648849924), (u'ratio', 0.03046064437385665), (u'thyroid', 0.023848613312963094), (u'boost', 0.022553830386287426), (u'nonparametr', 0.02121605069435915), (u'biomed', 0.019166254841288775), (u'featur_select', 0.016491278712993163), (u'imag_hash', 0.014250874692214869), (u'mixtur', 0.013545874530657797), (u'ga', 0.012677081897783896), (u'specifi', 0.012357526480068177), (u'basic', 0.011925863316429516), (u'prostat', 0.01177305574857011), (u'ultrasound_imag', 0.011584949063772594), (u'signatur_verif', 0.010972513446288685), (u'ode', 0.010661956932903506), (u'system', 0.00981525049055447), (u'data', 0.008748743253872824), (u'gabor', 0.008513274329816117), (u'arrang', 0.007222273013596076), (u'spoof_detect', 0.007094268624770328), (u'lbp', 0.004917537676838191), (u'discharg', 0.0045200634659358085), (u'ambigu', 0.003923321583542993), (u'esprit', 0.00389586549192057), (u'curvelet', 0.003666264912020545), (u'biolog_inspir', 0.0036363426491214743)]\n"
     ]
    }
   ],
   "source": [
    "print ldaModel.show_topic(131, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=100, id2word=dictionary)\n",
    "print lsimodel.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
